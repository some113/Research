import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Set;

import Config.Config;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;
import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class P2PHostIdentify {
    //P2P host detection threshold for detecting P2P flows
    private static int p2PHostDetectionThreshold = Config.BGPSupport;
    //Byte per packet threshold for merging flows
    private static int bytePerPacketThreshold = Config.NumberOfByteThread;

    private static int NumberOfPacketThread = Config.NumberOfPacketThread;

    private static String P2PFlowFileName = Config.P2PFlowFileName;

    private static int T = Config.T;

    private static BufferedWriter writer = null;

    //Mapper class for P2P host detection Map-Reduce Module
    public static class P2PHostDetectionMapper extends Mapper<Object, Text, Text, Text> {
        @Override
        protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {
//            System.out.println("Value in mapper: " + value);
            String[] parts = value.toString().split(" ");
            float time = Float.parseFloat(parts[0]);
            String srcAdd = parts[2], dstAdd = parts[3], flow = parts[4];
            srcAdd = srcAdd.substring(0, key.toString().lastIndexOf("."));
            dstAdd = dstAdd.substring(0, dstAdd.lastIndexOf("."));
            // TODO: Remove and process IP6 too
            if (srcAdd.contains(":") || dstAdd.contains(":")) {
                return;
            }
            context.write(new Text(srcAdd), new Text("" + time + " " + dstAdd + " " + flow));
        }
    }

    //Reducer class for P2P host detection Map-Reduce Module
    public static class P2PHostDetectionReducer extends Reducer<Text, Text, Text, Text> {

        HashMap<String, BufferedWriter> writerMap = new HashMap<String, BufferedWriter>(); //<prefix, writer>

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context)
                throws IOException, InterruptedException {
//            System.out.println("Key value: " + key);
            Set<String> dstSet = new HashSet<String>();
            File dir = new File(System.getProperty("user.dir") + "/OutputData/Sequences/");
            if (!dir.exists()) {
                dir.mkdir();
            }

            for (Text val : values) {
                String[] parts = val.toString().split("\\.");
                String prefix = parts[0] + "." + parts[1]+ "." + parts[2];
                if (!dstSet.contains(prefix)) {
                    dstSet.add(prefix);
                }
                if (dstSet.size() < p2PHostDetectionThreshold) {
                    return;
                }
            }
//            if (dstSet.size() < p2PHostDetectionThreshold) {
//                return;
//            }
            
            for (Text val : values) {

                String[] parts = val.toString().split(" ");
                float endTime = Float.parseFloat(parts[0]);
                String dstAdd = parts[1], flow = parts[2], sizes[] = flow.split(",");
                if (sizes.length < NumberOfPacketThread) {
                    continue;
                }
                int sum = 0;
                for (String size : flow.split(",")) {
                    sum += Integer.parseInt(size);
                }
                if ((float)sum / sizes.length > bytePerPacketThreshold) {
                    continue;
                }

//                String srcAdd = key.toString().substring(0, key.toString().lastIndexOf("."));
//                dstAdd = dstAdd.substring(0, dstAdd.lastIndexOf("."));

                dir = new File(System.getProperty("user.dir") + "/OutputData/Sequences/" + srcAdd + "/");
                dir.mkdir();

                int i = (int) endTime / T;

                if (!writerMap.containsKey(srcAdd + i)) {
                    File file = new File(System.getProperty("user.dir") + "/OutputData/Sequences/" + srcAdd + "/seq" + i + ".txt");
                    writerMap.put(srcAdd + i, new BufferedWriter(new FileWriter(
                            System.getProperty("user.dir") + "/OutputData/Sequences/" + srcAdd + "/seq" + i + ".txt", true)));
                }
                writer = writerMap.get(srcAdd + i);

                writer.write(flow.replace(",", " -1 ") + " -1 -2\n");
                writer.flush();

                context.write(new Text(srcAdd), new Text(dstAdd + " " + flow));
            }
            writer.flush();
            //writer.close();
        }
    }



    public static void run() throws IllegalArgumentException, IOException {

        //Create MAP-REDUCE job for detecting P2P flows.
//        BasicConfigurator.configure();
        JobConf conf = new JobConf(P2PHostIdentify.class);

        //FileModifier.deleteDir(new File(PeerCatcherConfigure.ROOT_LOCATION  + "/p2p_host_detection"));
        org.apache.commons.io.FileUtils.deleteDirectory(new File(System.getProperty("user.dir") +"/OutputData"));
        conf.setJobName("p2p_host_detection");

        Job jobP2PHostDetection = Job.getInstance();
        jobP2PHostDetection.setJobName("Job_p2p_host_detection_");
        jobP2PHostDetection.setJarByClass(P2PHostIdentify.class);
        jobP2PHostDetection.setMapperClass(P2PHostDetectionMapper.class);
        jobP2PHostDetection.setReducerClass(P2PHostDetectionReducer.class);
        jobP2PHostDetection.setOutputKeyClass(Text.class);
        jobP2PHostDetection.setOutputValueClass(Text.class);
        ControlledJob ctrlJobP2PHostDetection = new ControlledJob(conf);
        ctrlJobP2PHostDetection.setJob(jobP2PHostDetection);

        FileInputFormat.addInputPath(jobP2PHostDetection,
                new Path(System.getProperty("user.dir") +"/InputData"));
//        FileInputFormat.addInputPath(jobP2PHostDetection,
//                new Path(PeerCatcherConfigure.ROOT_LOCATION + "/INPUT/P2P"));
        FileOutputFormat.setOutputPath(jobP2PHostDetection,
                new Path(System.getProperty("user.dir") +"/OutputData"));
        // set the number of tasks for the reduce part of the job
        jobP2PHostDetection.setNumReduceTasks(18);

        // Run job
        JobControl jobCtrl = new JobControl("ctrl_p2p_host_detection");
        jobCtrl.addJob(ctrlJobP2PHostDetection);
        Thread t = new Thread(jobCtrl);
        t.start();

        while (true) {
            if (jobCtrl.allFinished()) {
                System.out.println(jobCtrl.getSuccessfulJobList());
                jobCtrl.stop();
                writer.flush();
                writer.close();
                break;
            }
        }

    }

}
